{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10 - Secure Deep Learning Classification \n",
    "\n",
    "### Context \n",
    "\n",
    "\n",
    "Data is the driver behind Machine Learning. Companies, organisations or hospitals that create and collect data are able to build and train their own machine learning models in house. This allows them to offer it as a service (MLaaS) to outside collaborators which don't have access to as much data but still would like to benefit from this model to gain insights on their own data. This data might be extremely sensitive (think about you sending a picture of your skin to detect skin cancer) and can't be sent in clear to a server. In return, it is likely that the model is also too critical from a business perspective to be sent directly to the client who could potentially steal it.\n",
    "\n",
    "In this context, one possible solution is to encrypt both the model and the data and to perform the machine learning prediction in a completely encrypted setting. Several encryption schemes exist that allow for computation over encrypted data, among which Secure Multi-Party Computation (SMPC), Homomorphic Encryption (FHE/SHE) and Functional Encryption (FE). We will focus here on Multi-Party Computation (which have been introduced in Tutorial 5) which consists of private additive sharing and relies on the crypto protocols SecureNN and SPDZ, the details of which are given [in this excellent blog post](https://mortendahl.github.io/2017/09/03/the-spdz-protocol-part1/).\n",
    "\n",
    "The exact setting in this tutorial is the following: consider that you are the server and you have some data. You first define and train a model with this data. Second, you get in touch with a client which holds some data and would like to access your model to do some prediction. It encrypts this data by building private shares while you do the same with your model, and then you execute the private evaluation of your model. Finally the result of the prediction is sent back to the client in an encrypted way so that the server (you) learns nothing about the client's data.\n",
    "\n",
    "\n",
    "Author:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are PySyft imports. In particular we define remote workers `alice`, `bob` and `client` along with a last one, the `crypto_provider` who gives all the crypto primitive we may need (See Part 5 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- hook PyTorch ie add extra functionalities to support Secure & Federated Learning\n",
    "client = sy.VirtualWorker(hook, id=\"client\") # <-- define remote workers client, bob & alice\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")  # <-- and the crypto_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the setting of the learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 10\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.log_interval = 100\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and sending to workers\n",
    "\n",
    "In our setting, we assume that the server has access to some data to first train its model. Here it is the MNIST training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((mean,), (std,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, there is a client which would like to have predictions on some data from the server's model. This client encrypts its data by sharing it additively across two workers alice and bob. Here we use MNIST testing set and we simulate the fact that the client has initially the data by sending it to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((mean,), (std,))\n",
    "]))\n",
    "\n",
    "# The client which owns the data performs the data encryption through sharing\n",
    "# TODO: send the data first to the client which performs the sharing\n",
    "\n",
    "# We select 100 random samples\n",
    "idx = torch.randperm(test_dataset.data.size(0))[:100]\n",
    "\n",
    "# We need to normalize manually the data as we don't use a dataloader\n",
    "test_data = test_dataset.data[idx].float()\n",
    "test_targets = test_dataset.targets[idx]\n",
    "client_data = sy.BaseDataset(\n",
    "    (test_data - test_data.mean() + mean)*std/test_data.std(), test_targets\n",
    ")\n",
    "\n",
    "# We encrypt this test data through additive sharing\n",
    "client_data = client_data.fix_precision().share(alice, bob, crypto_provider=crypto_provider)\n",
    "\n",
    "# And split it into batches\n",
    "client_data_batches = list(zip(\n",
    "    torch.split(client_data.data, args.test_batch_size),\n",
    "    torch.split(client_data.targets, args.test_batch_size)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network specification\n",
    "Here is the network specification used by the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training\n",
    "The training is done locally so this is pure local PyTorch training, nothing special here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.243068\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.450231\n",
      "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.300983\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.131476\n",
      "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.187243\n",
      "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.121139\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.135926\n",
      "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.132270\n",
      "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.082172\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.083237\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.156906\n",
      "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.151654\n",
      "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.050181\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.233930\n",
      "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.213018\n",
      "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.214955\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.016033\n",
      "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.097433\n",
      "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.061815\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.114781\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.034603\n",
      "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.024764\n",
      "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.005897\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.013325\n",
      "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.035721\n",
      "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.020961\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.048751\n",
      "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.036683\n",
      "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.091354\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.032474\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.039718\n",
      "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.050345\n",
      "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.012659\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.017080\n",
      "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.003719\n",
      "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.032907\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.023061\n",
      "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.029923\n",
      "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.007250\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.037856\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.013034\n",
      "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.040249\n",
      "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.008573\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.073300\n",
      "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.006234\n",
      "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.052023\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.021358\n",
      "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.002026\n",
      "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.039554\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.017440\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now trained and ready to be provided as a service!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secure evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as the server, we send the model to the workers holding the data. Because the model is sensitive information (you've spent time optimizing it!), you don't want to disclose its weights so you secret share the model just like we did with the dataset earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test function performs the encrypted evaluation. The model weights, the data inputs, the prediction and the target used for scoring are encrypted!\n",
    "\n",
    "However, the syntax is very similar to pure PyTorch testing of a model, isn't it nice?!\n",
    "\n",
    "The only thing we decrypt from the server side is the final score at the end to verify predictions were on average good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, client_data_batches):\n",
    "    print('Computing...')\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(client_data_batches):\n",
    "            print('...')\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            score = pred.eq(target.view_as(pred)).sum()\n",
    "            n_correct += score\n",
    "            n_total += args.test_batch_size\n",
    "\n",
    "    n_correct = n_correct.get().float_precision().long().item()\n",
    "    \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        n_correct, n_total,\n",
    "        100. * n_correct / n_total))\n",
    "    \n",
    "    #TODO: get back the model to local floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "\n",
      "Test set: Accuracy: 98/100 (98%)\n",
      "\n",
      "CPU times: user 9.04 s, sys: 757 ms, total: 9.79 s\n",
      "Wall time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test(args, model, client_data_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have done a completely secure prediction: the weights of the server's model have not leaked to the client and the server has no information about the data input nor the classification output!\n",
    "\n",
    "Regarding performance, classifying one image takes **less than 0.1 second**, which outperforms significantly available benchmarks (like [1.06s for the Chameleon framework (see paper Table 4)](https://arxiv.org/pdf/1801.03239.pdf)) as far as the comparison holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have seen here how easy it is to leverage to perform secure Machine Learning and protect users data without having to be a crypto expert!\n",
    "\n",
    "More on this topic will come soon, especially on private encrypted training of neural networks, when a organisation resorts to external sensitive data to train its own model.\n",
    "\n",
    "If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways! \n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Pick our tutorials on GitHub!\n",
    "\n",
    "We made really nice tutorials to get a better understanding of what Federated and Privacy-Preserving Learning should look like and how we are building the bricks for this to happen.\n",
    "\n",
    "- [Checkout the PySyft tutorials](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! If you want to start \"one off\" mini-projects, you can go to PySyft GitHub Issues page and search for issues marked `Good First Issue`.\n",
    "\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "- [Donate through OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
